Whole k-fold eval mode
Source domain: 8, Target domain: 8, Cur_fold 0
Corpus: TOEFL
Starting new HTTPS connection (1): s3.amazonaws.com:443
https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/xlnet-base-cased-spiece.model HTTP/1.1" 200 0
loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model from cache at /home/minghongxia/.cache/torch/transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8
Loading these models for language: en (English):
=======================
| Processor | Package |
-----------------------
| tokenize  | ewt     |
=======================

Use device: gpu
Loading: tokenize
Done loading processors!
Loading these models for language: en (English):
=======================
| Processor | Package |
-----------------------
| tokenize  | ewt     |
=======================

Use device: gpu
Loading: tokenize
Done loading processors!
Loading these models for language: en (English):
=======================
| Processor | Package |
-----------------------
| tokenize  | ewt     |
=======================

Use device: gpu
Loading: tokenize
Done loading processors!
Starting new HTTPS connection (1): s3.amazonaws.com:443
https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/xlnet-base-cased-config.json HTTP/1.1" 200 0
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /home/minghongxia/.cache/torch/transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.69e5e35e0b798cab5e473f253752f8bf4d280ee37682281a23eed80f6e2d09c6
Model config XLNetConfig {
  "architectures": [
    "XLNetLMHeadModel"
  ],
  "attn_type": "bi",
  "bi_data": false,
  "bos_token_id": 1,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "do_sample": false,
  "dropout": 0.1,
  "end_n_top": 5,
  "eos_token_id": 2,
  "eos_token_ids": 0,
  "ff_activation": "gelu",
  "finetuning_task": null,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "mem_len": null,
  "model_type": "xlnet",
  "n_head": 12,
  "n_layer": 12,
  "num_beams": 1,
  "num_labels": 2,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 5,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 250
    }
  },
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "untie_r": true,
  "use_bfloat16": false,
  "vocab_size": 32000
}

Loading embeddings from: xlnet-base-cased
Model: DIS_Simple Avg Plus Sentence Ordering
Encoder: XLNet
Starting new HTTPS connection (1): s3.amazonaws.com:443
https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/xlnet-base-cased-config.json HTTP/1.1" 200 0
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /home/minghongxia/.cache/torch/transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.69e5e35e0b798cab5e473f253752f8bf4d280ee37682281a23eed80f6e2d09c6
Model config XLNetConfig {
  "architectures": [
    "XLNetLMHeadModel"
  ],
  "attn_type": "bi",
  "bi_data": false,
  "bos_token_id": 1,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "do_sample": false,
  "dropout": 0.1,
  "end_n_top": 5,
  "eos_token_id": 2,
  "eos_token_ids": 0,
  "ff_activation": "gelu",
  "finetuning_task": null,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "mem_len": null,
  "model_type": "xlnet",
  "n_head": 12,
  "n_layer": 12,
  "num_beams": 1,
  "num_labels": 2,
  "num_return_sequences": 1,
  "output_attentions": true,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 5,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 250
    }
  },
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "untie_r": true,
  "use_bfloat16": false,
  "vocab_size": 32000
}

Starting new HTTPS connection (1): s3.amazonaws.com:443
https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin HTTP/1.1" 200 0
loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin from cache at /home/minghongxia/.cache/torch/transformers/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac
Starting new HTTPS connection (1): s3.amazonaws.com:443
https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/xlnet-base-cased-config.json HTTP/1.1" 200 0
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /home/minghongxia/.cache/torch/transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.69e5e35e0b798cab5e473f253752f8bf4d280ee37682281a23eed80f6e2d09c6
Model config XLNetConfig {
  "architectures": [
    "XLNetLMHeadModel"
  ],
  "attn_type": "bi",
  "bi_data": false,
  "bos_token_id": 1,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "do_sample": false,
  "dropout": 0.1,
  "end_n_top": 5,
  "eos_token_id": 2,
  "eos_token_ids": 0,
  "ff_activation": "gelu",
  "finetuning_task": null,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "mem_len": null,
  "model_type": "xlnet",
  "n_head": 12,
  "n_layer": 12,
  "num_beams": 1,
  "num_labels": 2,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 5,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 250
    }
  },
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "untie_r": true,
  "use_bfloat16": false,
  "vocab_size": 32000
}

Optimizer: adam
Optimizer: adam
**** Training Begins ****
**** Epoch 0/20 ****
Use CrossEntropyLoss
15/33-(1.815)
30/33-(1.482)
12/33-(1.240)
27/33-(1.263)

=== Evaluating Model ===
accuracy on Valid 0.5922619047619048

Best accuracy on Valid 0.5922619047619048
Total valid loss 0.8298966033118111
accuracy on Test 0.5952380952380952
Best accuracy on Test 0.5952380952380952

**** Epoch 0/20 ****
9/33-(1.324)
24/33-(1.306)
6/33-(0.989)
21/33-(1.142)

=== Evaluating Model ===
accuracy on Valid 0.6696428571428571

Best accuracy on Valid 0.6696428571428571
Total valid loss 0.6842146373930431
accuracy on Test 0.7053571428571429
Best accuracy on Test 0.7053571428571429

**** Epoch 1/20 ****
3/33-(0.944)
18/33-(1.394)
0/33-(1.340)
15/33-(0.856)

=== Evaluating Model ===
accuracy on Valid 0.7202380952380952

Best accuracy on Valid 0.7202380952380952
Total valid loss 0.628159815356845
accuracy on Test 0.7202380952380952
Best accuracy on Test 0.7202380952380952

**** Epoch 2/20 ****
30/33-(0.652)
12/33-(0.936)
27/33-(1.606)
9/33-(0.661)

=== Evaluating Model ===
accuracy on Valid 0.6339285714285714

Total valid loss 0.8594993140016284
accuracy on Test 0.6428571428571429
Best accuracy on Test 0.7202380952380952

**** Epoch 3/20 ****
24/33-(0.680)
6/33-(0.769)
21/33-(0.656)
3/33-(0.766)
18/33-(0.857)

=== Evaluating Model ===
accuracy on Valid 0.5863095238095238

Total valid loss 0.7842777115958077
accuracy on Test 0.6458333333333334
Best accuracy on Test 0.7202380952380952

**** Epoch 4/20 ****
0/33-(0.880)
15/33-(0.461)
30/33-(0.768)
12/33-(0.750)

=== Evaluating Model ===
accuracy on Valid 0.6041666666666666

Total valid loss 0.7599238753318787
accuracy on Test 0.6666666666666666
Best accuracy on Test 0.7202380952380952

**** Epoch 5/20 ****
27/33-(0.597)
9/33-(0.629)
24/33-(0.578)
6/33-(0.819)

=== Evaluating Model ===
accuracy on Valid 0.6369047619047619

Total valid loss 0.7560887138048807
accuracy on Test 0.6636904761904762
Best accuracy on Test 0.7202380952380952

**** Epoch 6/20 ****
21/33-(0.698)
3/33-(0.829)
18/33-(1.116)
0/33-(0.483)

=== Evaluating Model ===
accuracy on Valid 0.6875

Total valid loss 0.6072482466697693
accuracy on Test 0.7380952380952381
Best accuracy on Test 0.7380952380952381

**** Epoch 7/20 ****
15/33-(0.414)
30/33-(0.584)
12/33-(0.263)
27/33-(0.492)

=== Evaluating Model ===
accuracy on Valid 0.7083333333333334

Total valid loss 0.5776215323380062
accuracy on Test 0.7589285714285714
Best accuracy on Test 0.7589285714285714

**** Epoch 8/20 ****
9/33-(0.615)
24/33-(0.755)
6/33-(0.660)
21/33-(0.650)
3/33-(0.390)

=== Evaluating Model ===
accuracy on Valid 0.7113095238095238

Total valid loss 0.5956062667426609
accuracy on Test 0.7619047619047619
Best accuracy on Test 0.7619047619047619

**** Epoch 9/20 ****
18/33-(0.239)
0/33-(0.208)
15/33-(0.655)
30/33-(0.288)

=== Evaluating Model ===
accuracy on Valid 0.7142857142857143

Total valid loss 0.6076125346478962
accuracy on Test 0.7321428571428571
Best accuracy on Test 0.7619047619047619

**** Epoch 10/20 ****
12/33-(0.200)
27/33-(0.760)
9/33-(1.545)
24/33-(1.075)

=== Evaluating Model ===
accuracy on Valid 0.6696428571428571

Total valid loss 0.6676408322084517
accuracy on Test 0.7053571428571429
Best accuracy on Test 0.7619047619047619

**** Epoch 11/20 ****
6/33-(0.672)
21/33-(0.647)
3/33-(0.389)
18/33-(0.303)

=== Evaluating Model ===
accuracy on Valid 0.6636904761904762

Total valid loss 0.8814979663916996
accuracy on Test 0.6964285714285714
Best accuracy on Test 0.7619047619047619

**** Epoch 12/20 ****
0/33-(0.449)
15/33-(0.619)
30/33-(0.420)
12/33-(0.332)

=== Evaluating Model ===
accuracy on Valid 0.6398809523809523

Total valid loss 0.9074121451094037
accuracy on Test 0.6994047619047619
Best accuracy on Test 0.7619047619047619

**** Epoch 13/20 ****
27/33-(0.642)
9/33-(0.183)
24/33-(0.541)
6/33-(0.309)
21/33-(0.148)

=== Evaluating Model ===
accuracy on Valid 0.6458333333333334

Total valid loss 1.098382545369012
accuracy on Test 0.6369047619047619
Best accuracy on Test 0.7619047619047619

**** Epoch 14/20 ****
3/33-(0.104)
18/33-(0.195)
0/33-(0.282)
15/33-(0.666)

=== Evaluating Model ===
accuracy on Valid 0.7083333333333334

Total valid loss 1.0057955313296545
accuracy on Test 0.7053571428571429
Best accuracy on Test 0.7619047619047619

**** Epoch 15/20 ****
30/33-(0.431)
12/33-(0.125)
27/33-(0.289)
9/33-(0.491)

=== Evaluating Model ===
accuracy on Valid 0.6934523809523809

Total valid loss 1.1737137536207836
accuracy on Test 0.6785714285714286
Best accuracy on Test 0.7619047619047619

**** Epoch 16/20 ****
24/33-(0.358)
6/33-(0.348)
21/33-(0.510)
3/33-(0.461)

=== Evaluating Model ===
accuracy on Valid 0.6994047619047619

Total valid loss 0.8123137149072829
accuracy on Test 0.7291666666666666
Best accuracy on Test 0.7619047619047619

**** Epoch 17/20 ****
18/33-(0.553)
0/33-(0.113)
15/33-(0.173)
30/33-(0.449)

=== Evaluating Model ===
accuracy on Valid 0.7023809523809523

Total valid loss 1.0564344546624593
accuracy on Test 0.7113095238095238
Best accuracy on Test 0.7619047619047619

**** Epoch 18/20 ****
12/33-(0.284)
27/33-(0.069)
9/33-(0.145)
